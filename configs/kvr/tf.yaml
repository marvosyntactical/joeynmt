name: "kb-transformer-proper"

data:
    src: "usr"
    trg: "carno"

    # kb start 

    kb_task: "YES"
    kb_src: "kbkFINAL"
    kb_trg: "kbcFINAL"
    kb_lkp: "lkp"
    kb_len: "lenFINAL"
    kb_truvals: "trvFINAL"
    global.trv: "global.trvFINAL"
    trutrg: "car"

    # kb end

    train: "data/kvr/train"
    dev: "data/kvr/dev"
    test: "data/kvr/test"
    level: "word"
    lowercase: True
    max_sent_length: 128

testing:
    beam_size: 5
    alpha: 1.0

training:
    #random_seed: 42
    optimizer: "adam"
    normalization: "tokens"
    adam_betas: [0.9, 0.999]
    scheduling: "plateau"
    patience: 5
    decrease_factor: 0.7
    loss: "crossentropy"
    learning_rate: 0.0003
    learning_rate_min: 0.00000001
    weight_decay: 0.0
    label_smoothing: 0.1
    batch_size: 1
    batch_type: "token"
    early_stopping_metric: "eval_metric"
    epochs: 100
    validation_freq: 5
    logging_freq: 100
    eval_metric: "bleu"
    model_dir: "models/tf"
    overwrite: True
    shuffle: True
    use_cuda: False 
    max_output_length: 128
    print_valid_sents: [0,1,2,3,145,148,151,154,157,160,164,297,298,299,307,308,309,310,311,312,313,314,315,410,420,430,440,450,460,500,501,502,550,551,553]    # print these validation sentences during each validation run, default: [0, 1, 2]
    keep_last_ckpts: 5
    
    # scheduled sampling defaults (=> teacher forcing):
    #type = linear, k = 1, c = 0. (e. doesnt matter in this case)
    sched_sampl_type: "linear"      # scheduled sampling type, must be one of linear(default) or exponential or invsigmoid
    sched_sampl_k: 1.               # k value for sched sampl, cf. https://arxiv.org/abs/1506.03099 Section 2.4
    sched_sampl_c_e: [0., 0.2]      # c (negative slope) and e (minimal truth) values for sched sampl

model:
    kb: True                        # kb decoder to be used or not?
    initializer: "xavier"
    embed_initializer: "xavier"
    embed_init_gain: 1.0
    init_gain: 1.0
    bias_initializer: "zeros"
    tied_embeddings: False
    tied_softmax: True
    encoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4
        embeddings:
            embedding_dim: 64
            scale: True
            dropout: 0.
        # typically ff_size = 4 x hidden_size
        hidden_size: 64
        ff_size: 256
        dropout: 0.0
    decoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4
        embeddings:
            embedding_dim: 64
            scale: True
            dropout: 0.
        # typically ff_size = 4 x hidden_size
        hidden_size: 64
        ff_size: 256 
        dropout: 0.0
