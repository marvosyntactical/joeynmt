name: "kb-transformer-proper"

data:
    src: "usr"
    trg: "carno"

    # kb start 

    kb_task: "YES"
    kb_src: "kbkNODEFAULT"
    kb_trg: "kbcNODEFAULT"
    kb_lkp: "lkp"
    kb_len: "lenNODEFAULT"
    kb_truvals: "trvNODEFAULT"
    global.trv: "global.trvNODEFAULT"
    trutrg: "car"

    # kb end

    train: "data/kvr/train"
    dev: "data/kvr/dev"
    test: "data/kvr/test"
    level: "word"
    lowercase: True
    max_sent_length: 128

testing:
    beam_size: 5
    alpha: 1.0

training:
    #random_seed: 42
    optimizer: "adam"
    normalization: "tokens"
    adam_betas: [0.9, 0.999]
    scheduling: "plateau"
    patience: 5
    decrease_factor: 0.7
    loss: "crossentropy"
    learning_rate: 0.0003
    learning_rate_min: 0.00000001
    weight_decay: 0.0
    label_smoothing: 0.1
    batch_size: 1
    batch_type: "token"
    early_stopping_metric: "eval_metric"
    epochs: 100
    validation_freq: 800
    logging_freq: 100
    eval_metric: "bleu"
    model_dir: "models/146_tf2_29-09" #
    overwrite: False
    shuffle: True
    use_cuda: True
    max_output_length: 128
    print_valid_sents: [0,1,2,3]
    keep_last_ckpts: 5
    sched_sampl_type: "linear"      # scheduled sampling type, must be one of linear(default) or exponential or invsigmoid
    sched_sampl_k: 1.               # k value for sched sampl, cf. https://arxiv.org/abs/1506.03099 Section 2.4
    sched_sampl_c_e: [0.0, 1.0]     # c (negative slope) and e (minimal truth) values for sched sampl

model:
    kb: True                        # kb decoder used or not?
    k_hops: 1                       # num of kvr att layers
    do_postproc: True               # do postprocessing in inference (use False and trutrg = car to report cheated metrics)
    kb_max_dims: [256]              # max num of entries per KB dimension,e.g. [16, 32] for subj x relation or [256] for flat KB (eric et al)
    copy_from_source: True
    kb_input_feeding: True
    # kb_feed_rnn: True
    tfstyletf: True 


    initializer: "xavier"
    embed_initializer: "xavier"
    embed_init_gain: 1.0
    init_gain: 1.0
    bias_initializer: "zeros"
    tied_embeddings: False
    tied_softmax: True
    encoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4
        embeddings:
            embedding_dim: 64
            scale: True
            dropout: 0.
        # typically ff_size = 4 x hidden_size
        hidden_size: 64
        ff_size: 256
        dropout: 0.0
    decoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4
        embeddings:
            embedding_dim: 64
            scale: True
            dropout: 0.
        # typically ff_size = 4 x hidden_size
        hidden_size: 64
        ff_size: 256 
        dropout: 0.0
