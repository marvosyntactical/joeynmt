name: "❤️"

# This configuration serves the purpose of documenting and explaining the kb settings, *NOT* as an example for good hyperparamter settings.

data: # specify your data here
    src: "usr"                             # src language: expected suffix of train files, e.g. "train.de"
    trg: "carno"                           # trg language
    
    kb_task: "YES"                         # KVR: assig this with boolean val of True if KVR task performed 
    kb_src: "kbkNODEFAULT"                 # KVR: kb keys (subj pad rel) extension; see add_canon...py script for alternative extension; atm either of kb, can
    kb_trg: "kbcNODEFAULT"                 # KVR: kb canon vals extension; see add_canon...py script for alternative extension; atm either of kb, can
    kb_lkp: "lkp"                          # KVR: kb association lkp file ext
    kb_len: "lenNODEFAULT"                 # KVR: kb lengths file ext
    kb_truvals: "trvNODEFAULT"             # KVR: file extension for truvals
    trutrg: "carno"                        # KVR: file extension for uncanonized target data (for valid/test postprocessing)
    global_trv: "global.trvNODEFAULT"      # KVR: concatenation of train/dev/test trv files for lookup vocab building
    
    train: "data/kvr/train"                # training data
    dev: "data/kvr/dev"                    # development data for validation
    test: "data/kvr/test"                  # test data for testing final model; optional
    level: "word"                          # segmentation level: either "word", "bpe" or "char"
    lowercase: True                        # lowercase the data, also for validation
    max_sent_length: 128                   # filter out longer sentences from training (src+trg)
    src_voc_min_freq: 1                    # src minimum frequency for a token to become part of the vocabulary
    #src_voc_limit: 100                    # src vocabulary only includes this many most frequent tokens, default: unlimited
    trg_voc_min_freq: 1                    # trg minimum frequency for a token to become part of the vocabulary
    #trg_voc_limit: 100                    # trg vocabulary only includes this many most frequent tokens, default: unlimited
testing:                                   # specify which inference algorithm to use for testing (for validation it's always greedy decoding)
    beam_size: 5                           # size of the beam for beam search TODO
    alpha: 1.0                             # length penalty for beam search

training:                                  # specify training details here
    #load_model: "models/40_kbtf_17-08/best.ckpt" # if given, load a pre-trained model from this checkpoint
    random_seed: 69                        # set this seed to make training deterministic
    optimizer: "adam"                      # choices: "sgd", "adam", "adadelta", "adagrad", "rmsprop", default is SGD
    normalization: "tokens"                # NOTE what is this
    adam_betas: [0.9, 0.999]               # beta parameters for Adam. These are the defaults. Typically these are different for Transformer models.
    learning_rate: 0.0003                  # initial learning rate, default: 3.0e-4
    learning_rate_min: 1.0e-8              # stop learning when learning rate is reduced below this threshold, default: 1.0e-8
    #learning_rate_factor: 1               # factor for Noam scheduler (used with Transformer)
    #learning_rate_warmup: 4000            # warmup steps for Noam scheduler (used with Transformer)
    #clip_grad_val: 10.0                   # clip the gradients to this value when they exceed it, optional
    #clip_grad_norm: 1.0                   # norm clipping instead of value clipping
    weight_decay: 0.0                      # l2 regularization, default: 0
    batch_size: 1                          # mini-batch size as number of sentences (when batch_type is "sentence"; default) or total number of tokens (when batch_type is "token")
    batch_type: "token"                    # create batches with sentences ("sentence", default) or tokens ("token")
    eval_batch_size: 10                    # mini-batch size for evaluation (see batch_size above)
    eval_batch_type: "token"               # evaluation batch type ("sentence", default) or tokens ("token")
    batch_multiplier: 1                    # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches
    scheduling: "plateau"                  # learning rate scheduling, optional, if not specified stays constant, options: "plateau", "exponential", "decaying", "noam" (for Transformer)
    patience: 5                            # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate
    decrease_factor: 0.7                   # specific to plateau & exponential scheduler: decrease the learning rate by this factor
    epochs: 1000                           # train for this many epochs
    validation_freq: 500                   # validate after this many updates (number of mini-batches), default: 1000
    logging_freq: 10                       # log the training progress after this many updates, default: 100
    eval_metric: "bleu"                    # validation metric, default: "bleu", other options: "chrf", "token_accuracy", "sequence_accuracy"
    early_stopping_metric: "eval_metric"   # when a new high score on this metric is achieved, a checkpoint is written, when "eval_metric" (default) is maximized, when "loss" or "ppl" is minimized
    model_dir: "models/101_tf2000_23-09" # directory where models and validation results are stored, required
    overwrite: False                       # overwrite existing model directory, default: False. Do not set to True unless for debugging!
    shuffle: True                          # shuffle the training data, default: True
    use_cuda: True                         # use CUDA for acceleration on GPU, required. Set to False when working on CPU.
    #max_output_length: 31                 # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length
    print_valid_sents: [0,1,2,3,297,298,299,307,308,309,310,311,312,313,314,315,410,420,430,440,450,460,500,501,502,550,551,553]    # print these validation sentences during each validation run, default: [0, 1, 2]
    keep_last_ckpts: 5                     # keep this many of the latest checkpoints, if -1: all of them, default: 5
    label_smoothing: 0.1                   # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)

model:                                     # specify your model architecture here
    kb: True                               # kb decoder used or not?
    k_hops: 1                              # num of kvr att layers
    do_postproc: False                     # do postprocessing in inference (use False and trutrg = car to report cheated metrics)
    kb_max_dims: [256]                     # max num of entries per KB dimension,e.g. [16, 32] for subj x relation or [256,] for flat KB (eric et al)
    copy_from_source: True #
    kb_input_feeding: True #
    kb_feed_rnn: True #


    initializer: "xavier"                  # initializer for all trainable weights (xavier, zeros, normal, uniform)
    init_gain: 1.0                         # gain for Xavier initializer (default: 1.0)
    bias_initializer: "zeros"              # initializer for bias terms (xavier, zeros, normal, uniform)
    embed_initializer: "xavier"            # initializer for embeddings (xavier, zeros, normal, uniform)
    embed_init_gain: 1.0                   # gain for Xavier initializer for embeddings (default: 1.0)
    init_rnn_orthogonal: False             # use orthogonal initialization for recurrent weights (default: False)
    tied_embeddings: False                 # tie src and trg embeddings, only applicable if vocabularies are the same, default: False
    tied_softmax: False                    # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False
    encoder:
        type: "transformer"                # encoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
        embeddings:
            embedding_dim: 512             # size of embeddings
            scale: True                    # scale the embeddings by sqrt of their size, default: False
            dropout: 0.
        hidden_size: 512                   # size of RNN
        bidirectional: True                # use a bi-directional encoder, default: True
        num_layers: 6                      # stack this many layers of equal size, default: 1
        num_heads: 4
        freeze: False                      # if True, encoder parameters are not updated during training (does not include embedding parameters)
        ff_size: 1024
        dropout: 0.0
    decoder:
        type: "transformer"                # decoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
        embeddings:
            embedding_dim: 512 
            scale: True 
            dropout: 0.
        hidden_size: 512 
        ff_size: 1024
        dropout: 0.0
        num_layers: 6
        num_heads: 4
